Dynamic partition Working principles

Physical machines are members of a Batch System cluster (LSF) and a
Cloud (Openstack) cluster at the same time; furthermore, they can only be
active in a mutually exclusive manner on one and one only cluster at
any time. This implicitly defines a partition of hosts active at batch
or cloud side.

We refer to active Hosts in the batch partition as "Worker Nodes", and
to active hosts in the cloud partition as "Compute Nodes".

A Partition driver is a software component dealing with transition
requests: it performs the needed steps to convert a host from WN to CN
or from CN to WN. Note that nodes are not removed from one cluster, nor
joined the another; they are simply made active on the target cluster and
inactive in the origin cluster.

To do so:

- At cloud side

-- Compute Nodes are Enabled / Disabled using OpenStack APIs

- At Batch side

-- Worker Nodes are configured to publish a numeric status value
   (External Load Index) named dynp, indicating the partition they
   belong to.

-- LSF Master is configured to alter job parameters at submission time,
   adding the requirement for a node having the correct value of dynp



Deployment instructions

1. Assumptions

This guide assumes that:

- A properly configured Batch System instance is operational
  (currently LSF, version 7.x or newer)

- A working OpenStack instance is in place, with a Cloud Controller as
  privileged member of the LSF cluster.

- Cluster members (WN and CN) have read access to a shared filesystem
 ( mounted as LSF_TOP=/usr/share/lsf for the sake of example )


2. Deploy scripts and configurations

from the LSF master:

Ensure that needed path exist or create them:



- mkdir -p $LSF_TOP/var/tmp/cloudside/
- mkdir -p $LSF_TOP/var/tmp/batchside/
- mkdir -p $LSF_TOP/conf/scripts/dynpart/

cp dynpart.conf elim.dynp esub.dynp mcjobs_r.c $LSF_TOP/conf/scripts/dynpart/
cp farm.json $LSF_TOP/var/tmp/cloudside/

ln -s $LSF_TOP/conf/scripts/dynpart/elim.dynp $LSF_SERVERDIR/elim.dynp
ln -s $LSF_TOP/conf/scripts/dynpart/esub.dynp $LSF_SERVERDIR/esub.dynp

Compile the C program

The mcjobs_r.c C program queries LSF through its APIs to retrieve the
list of running jobs on each host. Pre compiled binary cannot be
distributed due to licensing constraints, thus it must be compiled
locally.  Following is an example compile command, on LSF9.1; adapt to
your specific setup.

cd $LSF_TOP/conf/scripts/dynpart/
gcc mcjobs_r.c -I/usr/share/lsf/9.1/include/ /usr/share/lsf/9.1/linux2.6-glibc2.3-x86_64/lib/libbat.a /usr/share/lsf/9.1/linux2.6-glibc2.3-x86_64/lib/liblsf.a -lm -lnsl -ldl -o mcjobs_r


Edit dynpart.conf
Refer to doc/howto_configure_dynpart.conf

Edit LSF configuration files

/usr/share/lsf/conf/lsf.cluster.<clustername>

In the Host section specify usage of the dynp elim on each WN
participating in the dynamic partitioning

Following is an example Host section:

Begin   Host
HOSTNAME  model    type        server r1m  mem  swp  RESOURCES    #Keywords

# lsf master
lsf9test   !   !   1   3.5   ()   ()   (mg)

#Cloud Controller for Dynamic Partitioning
t1-cloudcc-02   ! ! 1 3.5 () () (mg)

#Worker nodes
wn-206-01-01-01-b ! ! 1 3.5 () () (dynp)
wn-206-01-01-02-b ! ! 1 3.5 () () (dynp)

End     Host

################

Define the dynp External Load Index In the Resource Section of lsf.shared:

Begin Resource
RESOURCENAME  TYPE    INTERVAL INCREASING  DESCRIPTION        # Keywords

   dynp    Numeric 60      Y        (dynpart: 1 batch, 2 cloud)

[....]
End Resource

################

Declare use of the custom ESUB method. Add the following in lsf.conf:

LSB_ESUB_METHOD="dynp"

Note: The provided esub.dynp assumes that no other esub method is in place. If so, you must adapt it to your specific case.

###############

Verify LSF configuration is ok:

lsadmin ckconfig

If everything is ok (no errors found) reconfigure and restart lim on
all nodes in the cluster:

[root@lsf9test ~]# lsadmin reconfig

Checking configuration files ...
No errors found.

Restart only the master candidate hosts? [y/n] n
Do you really want to restart LIMs on all hosts? [y/n] y
Restart LIM on <lsf9test> ...... done
Restart LIM on <t1-cloudcc-02> ...... done
Restart LIM on <wn-206-01-01-01-b> ...... done
Restart LIM on <wn-206-01-01-02-b> ...... done

Note: you can only manually restart lim on a subset of nodes, if
needed. For example, if you configure dynp for more nodes in
lsf.cluster.<clustername> cluster and want to make them partition
aware you can restrict limrestart to those nodes only.

Next, restart the Master Batch Daemon

[root@lsf9test ~]# badmin mbdrestart

Little after the header output line of the lsload -l command
will display the new External Load Information value dynp.

Time after (limrestart takes several minutes to take effect, even on a
small cluster) the value 1 should be reported by each node configured
to play dynp. other cluster members would display a dash.

At Cloud side
<to be written>

##########

Now everything should be ready. You can chek your setup following
instructions on dynpart_test.txt



